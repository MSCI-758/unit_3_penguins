---
title: "3.3: t-tests"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, collapse=TRUE) 
```

### Unit 3: Penguins
#### Lesson 3: t-tests
#### New functions: 
identify_outliers(), stat_qq(), t.test(), t_test(), droplevels(), levene_test()

***

### Intro to t-tests

t-tests are used to assess the difference between two means. When conducting a t-test, you are testing the null hypothesis (H0) that the means are the same. As a standard practice in classical (frequentist) statistics, if the p-value resulting from your t-test is < 0.05, you reject the null hypothesis in favor of the alternative hypothesis, which states that the means are significantly different. 

There are 3 types of t-test:

-  One-sample t-test
-  Independent sample t-test (unpaired two-sample t-test)
-  Paired t-test

We'll use the `tidyverse` and our palmer penguins data to run through an example of each of these types of t-test. We'll also load in the package `rstatix` because it contains pipe-friendly statistics functions that play nicely with the `tidyverse.` Don't forget to install the `rstatix` package if it is your first time using it! I'm also going to use the kable() function in the knitr package to print out some of the results tables neatly for this tutorial. This is just for aesthetics, and is not necessary for any of the plotting or statistical tests in this lesson.

```{r, message=FALSE}
library(tidyverse)
library(palmerpenguins)
library(rstatix)
library(knitr)  # prints pretty tables with RMarkdown
```

### One-sample t-test

The one-sample t-test, also known as the single-parameter t test or single-sample t-test, is used to compare the mean of one sample to a known standard (or theoretical / hypothetical) mean. Generally, the theoretical mean comes from somewhere in the literature or from a previous experiment in your own lab. 

The one-sample t-test assumes the following characteristics about the data:

-  No significant outliers in the data
-  Data should be approximately normally distributed

#### Exploratory data analysis

Before conducting any statistical analyses on your data, it's imperative that you understand your data. This sounds obvious, but it's a step that people all too often skip. Here are some great goals for exploratory data analysis:

1. Looking at the raw data values.
2. Computing summary statistics.
3. Creating data visualizations.

These are things we've been doing already in the course, but now that we are actually trying to run a statistical test with the penguins data, we should use the functions in base R and the tidyverse to formally run through these steps. We will be looking at the difference in body mass between the three penguin species in our observations: Gentoo, Chinstrap and Adelie.

```{r}
# Two great functions for looking at the first few rows of your variables:
head(penguins)
glimpse(penguins)

# Summary statistics. Note # of observations and NAs
summary(penguins)

ggplot(data=penguins) +
  geom_histogram(aes(x=body_mass_g, fill=species))
```

#### Example: Observed Gentoo body mass vs. literature value

Are the Gentoo penguin body mass observations collected in our palmer penguins dataset significantly different than the mean Gentoo penguin body mass accepted in the literature? We can use the body mass value from the Encyclopedia of Life:

https://eol.org/traitbank

Search for "Gentoo Penguin" in the search bar and you will find that the trait bank lists body mass as 5500g. Let's see what our Gentoo body mass observations look like:

```{r}
# Separate just the Gentoo from all the penguin data
gentoo = penguins %>% 
  filter(species=="Gentoo") 

# Calculate the mean and standard deviation Gentoo body mass in our data (sometimes base R is more sensible than dplyr)
mean(gentoo$body_mass_g, na.rm=TRUE)
sd(gentoo$body_mass_g, na.rm=TRUE)
```

Before we conduct our one-sample t-test, let's first check the assumptions. Are there any significant outliers in the data? We can plot a histogram and see if any of the data near the tails stand out:

```{r}
# Quickly visualize the body mass data
ggplot(data=gentoo) +
  geom_histogram(aes(x=body_mass_g))
```

We see a few body mass values at the tails, but nothing that stands out in an extreme way to indicate that the data was improperly coded or otherwise problematic.

The normality assumption can be checked by plotting the data in a Quantile-Quantile plot (QQ plot) and see if it mostly falls along the 1:1 line. A Normal Q-Q plot just shows your data, sorted and plotted along the x axis, versus quantiles from a standard Normal distribution on the y axis.

```{r}
# Check normality assumption with a qqplot:
ggplot(gentoo) +
  stat_qq(aes(sample=body_mass_g))
```

If the data are not normally distributed, it’s recommended to use a non-parametric test such as the one-sample Wilcoxon signed-rank test. This test is similar to the one-sample t-test, but focuses on the median rather than the mean.

Now let's do our one-sample t-test to see if our body mass data is significantly different from the body mass value of mu = 5500 g published in the literature:

```{r}
t.test(gentoo$body_mass_g, mu = 5500) # Base R

t_test_results = gentoo %>% t_test(body_mass_g ~ 1, mu = 5500) # dplyr-friendly version
kable(t_test_results) # prints results nicely with RMarkdown
```

The results of our one-sample t-test show the following components:

-  .y.: the outcome variable used in the test.
-  group1,group2: generally, the compared groups in the pairwise tests. Here, we have null model (one-sample test).
-  n: the number of data points in the sample
-  statistic: test statistic (t-value) used to compute the p-value.
-  df: degrees of freedom.
-  p: p-value.

The output p-value is much less than 0.05 (it's almost p=0). That means we reject our null hypothesis that our Gentoo body mass observations are similar to the literature value.

Huh. Interesting. Remember earlier we caculated our observed mean Gentoo body mass is 5076 g. If we were writing this test for a publication we could use language like this: "We conducted a one-sample t-test to compare the body mass of Gentoo penguins measured in the field with the theoretical mean adult Gentoo body mass of 5500 g pulled from the Encyclopedia of Life trait bank. The penguin body mass measurements in this study were significantly smaller than the theoretical mean (p=6.05x10^-16)." 

Maybe our penguins were hungry. If I had to guess, the Encyclopedia of Life body mass trait is probably for adult Gentoo penguins, and we include juveniles in our observations. Or perhaps the Encylcopedia value is junk. If this were actually my data and I wanted to publish on it, I'd use this as a jumping point to do some more research.

### Independent sample t-test:

The independent samples t-test (or unpaired samples t-test) is used to compare the mean of two independent groups. For example, you might want to compare the average weights of individuals grouped by gender: male and female groups, which are two unrelated/independent groups. 

Assumptions

-  Independence of the observations. There is no relationship between the observations in each group.
-  No significant outliers in the groups
-  the two groups of samples should be normally distributed.
-  If using the Student's t-test, the variances of the two groups should not be significantly different. This assumption is relaxed in the Welch’s t-test.

#### Example: Gentoo vs. Adelie body mass 

Let's use the independent sample t-test to see if there is a significant difference in the mean body mass of Gentoo penguins vs. Adelie penguins:

```{r}
# Simplify the dataset to what we need
data_for_t_test = penguins %>%
  filter(species %in% c("Gentoo", "Adelie"),
         !is.na(body_mass_g)) %>%
  select(species, body_mass_g) %>%
  droplevels() # removes "Chinstrap" level from the species factor / categorical variable

summary(data_for_t_test)

# Calculate summary stats
data_for_t_test %>%
  group_by(species) %>%
  summarize(mean=mean(body_mass_g), sd=sd(body_mass_g))

# Plot a quick histogram:
ggplot(aes(x=body_mass_g), data=data_for_t_test) +
  geom_histogram() +
  facet_wrap(~species)

# Check normality assumption with a qqplot:
ggplot(data_for_t_test) +
  stat_qq(aes(sample=body_mass_g)) +
  facet_wrap(~species, scales="free")

# Check equality of variances; Levene's test null hypothesis: variances are equal
data_for_t_test %>% levene_test(body_mass_g ~ species) # if p<0.05, variances are NOT equal
```

We are examining the distribution of body mass observations for Gentoo and Adelie penguins, and the histograms looked normal with no major outliers. The Q-Q plots looked fine - the data fall along the 1:1 line. Levene's test checks for equality of variances, and since p>0.05, we accept the null hypothesis that the variances are equal. That means we can use the Student's t-test if we want (we'll probably be "safe" and use Welch's t-test anyway). Whew. Stats are exhuasting, right?

Now let's actually run the independent sample t-test.

```{r}
# Base R version:
t.test(data_for_t_test$body_mass_g ~ data_for_t_test$species)

# dplyr-friendly version:
data_for_t_test %>% 
  t_test(body_mass_g ~ species) 
```

So we can reject the null hypothesis that the means are equal, and accept the alternative hypothesis. "Adelie body mass is significantly lower than Gentoo body mass (Welch's t-test, p<0.001)."

If we had wanted to run a Student's t-test, we could have just included a parameter in the t.test function `var.equal=TRUE`. I'm also showing both the base R `t.test()` and the dplyr-friendly `t_test` from the `rstatix` package. A nice thing about `t.test()` is that there is a lot of hand-holding in the results output (who doesn't love having their hand held in statistics?). An advantage of `t_test()` is that the output is in a simple table, so tranferring these results into a report is easier, especially if you have a bunch of t-tests to do and you can build a data frame with a new test result in each row.

### Paired sample t-test:

The paired sample t-test is used to compare the means of two related groups of samples. Put into another words, it’s used in a situation where you have two pairs of values measured for the same samples. For example, you might want to compare the average weight of 20 sea urchins before and after some experimental treatment. The paired t-test can be used to compare the mean weights before and after treatment.

Assumptions:

-  No significant outliers in the differences between groups
-  The difference of pairs should follow a normal distribution.

We don't have any good exmaples of paired data in our penguins dataset. However, conducting the paired t-test looks almost identical to the independent sample t-test, except that you test your assumptions for outliers and normality on the difference between the paired data (i.e. for each individual urchin, the difference is the weight before treatment minus the weight after treatment). Then run `t_test()` with the parameter `paired=TRUE`.

### Note on assumptions:

**Assessing normality:** With large enough samples size (n > 30) the violation of the normality assumption should not cause major problems (according to the central limit theorem). This implies that we can ignore the distribution of the data and use parametric tests. 

**Assessing equality of variances:** Homogeneity of variances can be checked using the Levene’s test. Note that, by default, the t_test() function does not assume equal variances; instead of the standard Student’s t-test, it uses the Welch t-test by default, which is the considered the safer one. To use Student’s t-test, set var.equal = TRUE. The two methods give very similar results unless both the group sizes and the standard deviations are very different.

In the situations where the assumptions are violated, non-parametric tests, such as Wilcoxon test, are recommended.

***

### Exercise 3.1

Are Adelie penguin flipper lengths significantly different between males and females? Do some exploratory data analysis. Compute summary statistics and plot histograms. Then conduct an independent sample t-test. What do your results show?

***


